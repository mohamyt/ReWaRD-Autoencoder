{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (1248159204.py, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 84\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from args import conf\n",
    "from DataLoader import Dataset_\n",
    "from Autoencoder import *\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "\n",
    "    # Processing time\n",
    "    starttime = time.time()\n",
    "    \n",
    "    # Option\n",
    "    args = conf()\n",
    "    print(args)\n",
    "\n",
    "    # GPUs\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    #to deterministic\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    \n",
    "    # Training settings\n",
    "    train_transform = transforms.Compose([transforms.RandomCrop((args.crop_size, args.crop_size)),\n",
    "                                          transforms.ToTensor()])\n",
    "    train_dataset = Dataset_(args.path2traindb, transform=train_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "    \n",
    "    if args.val:\n",
    "        val_transform = transforms.Compose([transforms.RandomCrop((args.crop_size, args.crop_size)),\n",
    "                                            transforms.ToTensor()])\n",
    "        val_dataset = Dataset_(args.path2valdb, transform=train_transform)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "\n",
    "    # Model & optimizer\n",
    "    model = Network(args)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    \n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        assert os.path.isfile(args.resume), \"=> no checkpoint found at '{}'\".format(args.resume)\n",
    "        print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load('checkpoint_epoch_1.pth.tar')\n",
    "\n",
    "        # Adjust the keys of state_dict if necessary\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        new_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            new_key = k.replace('module.', '')\n",
    "            new_state_dict[new_key] = v\n",
    "\n",
    "        # Load state_dict into the model\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "        train_losses = checkpoint.get('train_losses', [])\n",
    "        batch_losses = checkpoint.get('batch_losses', [])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\".format(args.resume, checkpoint['epoch']))\n",
    "    if not args.no_multigpu:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    if not 'train_losses' in locals(): #Resume on existing data if available\n",
    "        train_losses = []\n",
    "        batch_losses = []\n",
    "\n",
    "    # Training\n",
    "    num_epochs = args.epochs\n",
    "    model.to(device)\n",
    "\n",
    "    # Use mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Save checkpoint function\n",
    "    def save_checkpoint(state, filename):\n",
    "        torch.save(state, filename)\n",
    "        print(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for imgs in tqdm(iterable=train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            imgs = imgs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, imgs)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            batch_losses.append(loss.item() * imgs.size(0))\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        if (epoch + 1) % args.save_interval == 0:\n",
    "            checkpoint_filename = f\"checkpoint_epoch_{epoch+1}.pth.tar\"\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + args.start_epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'batch_losses': batch_losses,\n",
    "            }, checkpoint_filename)\n",
    "            print(f\"Model checkpoint saved at epoch {epoch+1}\")\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        # Clear unused memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    # Processing time\n",
    "    endtime = time.time()\n",
    "    interval = endtime - starttime\n",
    "    print(\"elapsed time = {0:d}h {1:d}m {2:d}s\".format(int(interval/3600), int((interval%3600)/60), int((interval%3600)%60)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m checkpoint_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43msave_checkpoint\u001b[49m({\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m'\u001b[39m: train_losses,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_losses\u001b[39m\u001b[38;5;124m'\u001b[39m: batch_losses,\n\u001b[0;32m      8\u001b[0m }, checkpoint_filename)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel checkpoint saved at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_filename = f\"checkpoint_epoch_{epoch+1}.pth.tar\"\n",
    "save_checkpoint({\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'batch_losses': batch_losses,\n",
    "}, checkpoint_filename)\n",
    "print(f\"Model checkpoint saved at epoch {epoch+1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
